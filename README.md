
# Investigating the Detection of ChatGPT-Generated Texts across Radiology Reports

In this project, we aim to analyze the differences between texts authored by humans and those generated by ChatGPT in the under-studied domain of medicine. Additionally, we test several language models to assess their ability to distinguish between medical texts generated by ChatGPT and those written by humans. We aim to answer the following questions: 

1) What are the differences in vocabulary features, lexical diversity, part-of-speech distributions, and dependency relations between human-written and ChatGPT-generated radiology reports, and what do these differences suggest about the writing styles and language patterns?
2) Does the word frequency distribution in human-written and AI-generated radiology reports follow Zipf's law, and what are the implications of the observed differences?
3) How effective are fine-tuned transformer-based models, specifically RoBERTa and ELECTRA, in detecting ChatGPT-generated radiology reports compared to other machine learning algorithms?
