{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqbZGh-dzzPb",
    "outputId": "099585b2-ebbc-4117-cdbf-51fc6000b6af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human unique word count: 11600, ChatGPT unique word count: 13478\n",
      "Human word count: 245066, ChatGPT word count: 993511\n",
      "Human stemmed unique word count: 10863, ChatGPT stemmed unique word count: 12582\n",
      "Human stemmed word count: 10863, ChatGPT stemmed word count: 12582\n",
      "Human sentence count: 12.73, ChatGPT sentence count: 12.52\n",
      "Human sentence length: 8.78, ChatGPT sentence length: 9.02\n",
      "Human text length: 111.39, ChatGPT text length: 112.9\n",
      "Human TTR: 0.047334187524993264, ChatGPT TTR: 0.013566029968465372\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[\\^\\\\w\\\\s]', '', text.lower())\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return words, stemmed_words\n",
    "\n",
    "def get_text_stats(texts):\n",
    "    sentence_counts, sentence_lengths, text_lengths = [], [], []\n",
    "    all_words, all_stemmed_words = [], []\n",
    "    for text in texts:\n",
    "        sentences = text.split('.')\n",
    "        sentence_counts.append(len(sentences))\n",
    "        for sentence in sentences:\n",
    "            sentence_lengths.append(len(sentence.split()))\n",
    "        text_words, text_stemmed_words = preprocess_text(text)\n",
    "        text_lengths.append(len(text_words))\n",
    "        all_words.extend(text_words)\n",
    "        all_stemmed_words.extend(text_stemmed_words)\n",
    "    word_count = len(all_words)\n",
    "    unique_word_count = len(set(all_words))\n",
    "    stemmed_word_count = len(Counter(all_stemmed_words))\n",
    "    stemmed_unique_word_count = len(set(all_stemmed_words))\n",
    "    ttr = unique_word_count / word_count  # estimating lexical diversity\n",
    "    return {\n",
    "        'sentence_count': round(np.mean(sentence_counts), 2),\n",
    "        'sentence_length': round(np.mean(sentence_lengths), 2),\n",
    "        'text_length': round(np.mean(text_lengths), 2),\n",
    "        'word_count': word_count,\n",
    "        'unique_word_count': unique_word_count,\n",
    "        'stemmed_word_count': stemmed_word_count,\n",
    "        'stemmed_unique_word_count': stemmed_unique_word_count,\n",
    "        'ttr': ttr\n",
    "    }\n",
    "\n",
    "data = pd.read_csv('all_data.csv')\n",
    "human_texts = data[data.labels == 0]['text'].values\n",
    "chatgpt_texts = data[data.labels == 1]['text'].values\n",
    "\n",
    "human_stats = get_text_stats(human_texts)\n",
    "chatgpt_stats = get_text_stats(chatgpt_texts)\n",
    "\n",
    "print(f\"Human unique word count: {human_stats['unique_word_count']}, ChatGPT unique word count: {chatgpt_stats['unique_word_count']}\")\n",
    "print(f\"Human word count: {human_stats['word_count']}, ChatGPT word count: {chatgpt_stats['word_count']}\")\n",
    "print(f\"Human stemmed unique word count: {human_stats['stemmed_unique_word_count']}, ChatGPT stemmed unique word count: {chatgpt_stats['stemmed_unique_word_count']}\")\n",
    "print(f\"Human stemmed word count: {human_stats['stemmed_word_count']}, ChatGPT stemmed word count: {chatgpt_stats['stemmed_word_count']}\")\n",
    "print(f\"Human sentence count: {human_stats['sentence_count']}, ChatGPT sentence count: {chatgpt_stats['sentence_count']}\")\n",
    "print(f\"Human sentence length: {human_stats['sentence_length']}, ChatGPT sentence length: {chatgpt_stats['sentence_length']}\")\n",
    "print(f\"Human text length: {human_stats['text_length']}, ChatGPT text length: {chatgpt_stats['text_length']}\")\n",
    "print(f\"Human TTR: {human_stats['ttr']}, ChatGPT TTR: {chatgpt_stats['ttr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Y1GcEIBjlX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
